{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d03f7ce-8f85-40c6-aef0-7faa7d0d971b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# SciPy is commonly available on Databricks runtimes; if not, install it.\n",
    "# If you prefer, run this in a separate cell:\n",
    "# %pip install scipy\n",
    "from scipy import stats  # Welch test + t distribution (ppf/sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b887f5b-76f4-4c77-a603-bbcdd4752398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "TABLE = \"workspace.pilot_class_data.fact_questionnaire_long\"\n",
    "\n",
    "# School grouping rule from your study design\n",
    "UNDER_SCHOOLS = {\"school_b\", \"school_e\"}\n",
    "RESOURCED_SCHOOLS = {\"school_c\", \"school_d\"}\n",
    "\n",
    "# Analysis scope\n",
    "Q_MIN, Q_MAX = 1, 12\n",
    "ALPHA = 0.05              # 95% CI\n",
    "REQUIRE_COMPLETE_Q1_12 = False   # set True if you want only respondents with all 12 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6682750-ea79-4245-b825-6f7ff01fb45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Load data from Unity Catalog / workspace table\n",
    "# Why: spark.table is the standard way to access a registered table as a DataFrame.\n",
    "# -----------------------------\n",
    "df = spark.table(TABLE)  # SparkSession.table returns a DataFrame \n",
    "\n",
    "expected_cols = {\"respondent_key\", \"ai_used\", \"school_id\", \"question_num\", \"score\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}. Found: {df.columns}\")\n",
    "\n",
    "# Ensure correct types and keep only relevant fields\n",
    "df = (\n",
    "    df.select(\n",
    "        F.col(\"respondent_key\"),\n",
    "        F.col(\"ai_used\"),\n",
    "        F.col(\"school_id\"),\n",
    "        F.col(\"question_num\").cast(\"int\").alias(\"question_num\"),\n",
    "        F.col(\"score\").cast(\"double\").alias(\"score\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17abd557-31d8-4744-9840-aec104bfb9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Add school_group (Under vs Resourced)\n",
    "# Why: Gap Reduction / Equity metrics need grouping by school resources.\n",
    "# -----------------------------\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"school_group\",\n",
    "        F.when(F.col(\"school_id\").isin(list(UNDER_SCHOOLS)), F.lit(\"Under\"))\n",
    "         .when(F.col(\"school_id\").isin(list(RESOURCED_SCHOOLS)), F.lit(\"Resourced\"))\n",
    "         .otherwise(F.lit(None)),\n",
    "    )\n",
    "    .filter(F.col(\"school_group\").isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f62b09-81ef-44dc-8130-e218729df4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) Restrict to Q1–Q12 and aggregate to respondent-level mean\n",
    "# Why: each respondent answered 12 items; using respondent mean avoids pseudo-replication.\n",
    "# -----------------------------\n",
    "q12 = df.filter((F.col(\"question_num\").between(Q_MIN, Q_MAX)) & F.col(\"score\").isNotNull())\n",
    "\n",
    "resp = (\n",
    "    q12.groupBy(\"respondent_key\", \"ai_used\", \"school_id\", \"school_group\")\n",
    "       .agg(\n",
    "           F.avg(\"score\").alias(\"resp_mean_q1_12\"),\n",
    "           F.countDistinct(\"question_num\").alias(\"n_answered_q1_12\"),\n",
    "       )\n",
    ")\n",
    "\n",
    "if REQUIRE_COMPLETE_Q1_12:\n",
    "    resp = resp.filter(F.col(\"n_answered_q1_12\") == (Q_MAX - Q_MIN + 1))\n",
    "\n",
    "# Bring the (small) respondent-level table to pandas for stats\n",
    "pdf = resp.toPandas()\n",
    "\n",
    "# Basic sanity\n",
    "if pdf.empty:\n",
    "    raise ValueError(\"No data after filtering. Check question_num range, score nulls, or school_id values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdfaad28-334a-440a-8fa2-19ba86aafbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4) Utility: Welch t-test + CI from summary stats\n",
    "# Why: Welch handles unequal variances and unequal sample sizes (common in field data).\n",
    "# SciPy’s ttest_ind(equal_var=False) is Welch’s t-test. \n",
    "# -----------------------------\n",
    "def welch_from_summary(mean_a, var_a, n_a, mean_b, var_b, n_b, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Returns Welch t-test results for (A - B) using summary stats.\n",
    "    \"\"\"\n",
    "    if n_a < 2 or n_b < 2:\n",
    "        return None\n",
    "\n",
    "    se = np.sqrt(var_a / n_a + var_b / n_b)\n",
    "\n",
    "    # Welch–Satterthwaite df for difference of means\n",
    "    num = (var_a / n_a + var_b / n_b) ** 2\n",
    "    den = ((var_a / n_a) ** 2) / (n_a - 1) + ((var_b / n_b) ** 2) / (n_b - 1)\n",
    "    df = num / den\n",
    "\n",
    "    t = (mean_a - mean_b) / se\n",
    "    p = 2 * stats.t.sf(np.abs(t), df)  # two-tailed p-value using survival function\n",
    "\n",
    "    tcrit = stats.t.ppf(1 - alpha / 2, df)\n",
    "    diff = mean_a - mean_b\n",
    "    ci_low = diff - tcrit * se\n",
    "    ci_high = diff + tcrit * se\n",
    "\n",
    "    return {\n",
    "        \"estimate\": diff,\n",
    "        \"t\": t,\n",
    "        \"df\": df,\n",
    "        \"p\": p,\n",
    "        \"se\": se,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479daeae-86d0-406f-978b-16454be7de84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5) Utility: Welch-style test for a linear contrast of 4 independent means\n",
    "# Why: Gap Reduction / Equity DiD is a 4-cell contrast of means; variance adds across cells.\n",
    "# Welch–Satterthwaite equation gives effective df for sum of variance terms. \n",
    "# -----------------------------\n",
    "def welch_linear_contrast(groups, weights, alpha=0.05):\n",
    "    \"\"\"\n",
    "    groups: list of dicts with keys {mean, var, n}\n",
    "    weights: same length; contrast estimate = sum_i w_i * mean_i\n",
    "\n",
    "    Variance term per group: v_i = (w_i^2) * var_i / n_i\n",
    "    df approx: (sum v_i)^2 / sum (v_i^2 / (n_i - 1))  (Welch–Satterthwaite)\n",
    "    \"\"\"\n",
    "    if len(groups) != len(weights):\n",
    "        raise ValueError(\"groups and weights must have same length\")\n",
    "\n",
    "    # Must have >=2 per group to estimate variance\n",
    "    if any(g[\"n\"] < 2 for g in groups):\n",
    "        return None\n",
    "\n",
    "    estimate = sum(w * g[\"mean\"] for w, g in zip(weights, groups))\n",
    "\n",
    "    v_terms = [ (w**2) * g[\"var\"] / g[\"n\"] for w, g in zip(weights, groups) ]\n",
    "    se = np.sqrt(sum(v_terms))\n",
    "\n",
    "    num = (sum(v_terms))**2\n",
    "    den = sum((v**2) / (g[\"n\"] - 1) for v, g in zip(v_terms, groups))\n",
    "    df = num / den\n",
    "\n",
    "    t = estimate / se\n",
    "    p = 2 * stats.t.sf(np.abs(t), df)\n",
    "\n",
    "    tcrit = stats.t.ppf(1 - alpha / 2, df)\n",
    "    ci_low = estimate - tcrit * se\n",
    "    ci_high = estimate + tcrit * se\n",
    "\n",
    "    return {\n",
    "        \"estimate\": estimate,\n",
    "        \"t\": t,\n",
    "        \"df\": df,\n",
    "        \"p\": p,\n",
    "        \"se\": se,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ecbee4-286e-4205-8250-2eb85836cb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6) Compute Experience Lift (overall): AI - Baseline\n",
    "# Why: this is your headline effect on learning experience (Q1–Q12).\n",
    "# -----------------------------\n",
    "def summary(series: pd.Series):\n",
    "    s = series.dropna().astype(float)\n",
    "    return {\"n\": int(s.shape[0]), \"mean\": float(s.mean()), \"var\": float(s.var(ddof=1))}\n",
    "\n",
    "sY = summary(pdf.loc[pdf[\"ai_used\"] == \"Y\", \"resp_mean_q1_12\"])\n",
    "sN = summary(pdf.loc[pdf[\"ai_used\"] == \"N\", \"resp_mean_q1_12\"])\n",
    "\n",
    "exp_lift = welch_from_summary(\n",
    "    mean_a=sY[\"mean\"], var_a=sY[\"var\"], n_a=sY[\"n\"],\n",
    "    mean_b=sN[\"mean\"], var_b=sN[\"var\"], n_b=sN[\"n\"],\n",
    "    alpha=ALPHA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3f0038-815d-4647-800c-b680425e200f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7) Compute Equity metrics: Gap Reduction and Equity DiD (absolute)\n",
    "# Define cell means:\n",
    "#   UY = Under + AI, UN = Under + NoAI, RY = Resourced + AI, RN = Resourced + NoAI\n",
    "#\n",
    "# Equity Gap (Baseline) = RN - UN\n",
    "# Equity Gap (AI)       = RY - UY\n",
    "# Gap Reduction         = (RN - UN) - (RY - UY)\n",
    "#\n",
    "# Equity DiD (Under lift - Resourced lift)\n",
    "# = (UY - UN) - (RY - RN) = same algebraically as Gap Reduction (abs)\n",
    "# -----------------------------\n",
    "cell = (\n",
    "    pdf.groupby([\"ai_used\", \"school_group\"])[\"resp_mean_q1_12\"]\n",
    "       .agg([\"count\", \"mean\", lambda x: x.var(ddof=1)])\n",
    "       .rename(columns={\"<lambda_0>\": \"var\"})\n",
    ")\n",
    "\n",
    "def get_cell(ai_used, grp):\n",
    "    row = cell.loc[(ai_used, grp)]\n",
    "    return {\"n\": int(row[\"count\"]), \"mean\": float(row[\"mean\"]), \"var\": float(row[\"var\"])}\n",
    "\n",
    "RN = get_cell(\"N\", \"Resourced\")\n",
    "UN = get_cell(\"N\", \"Under\")\n",
    "RY = get_cell(\"Y\", \"Resourced\")\n",
    "UY = get_cell(\"Y\", \"Under\")\n",
    "\n",
    "gap_baseline = RN[\"mean\"] - UN[\"mean\"]\n",
    "gap_ai = RY[\"mean\"] - UY[\"mean\"]\n",
    "gap_reduction_abs = gap_baseline - gap_ai\n",
    "gap_reduction_pct = (gap_reduction_abs / gap_baseline) if gap_baseline != 0 else np.nan\n",
    "\n",
    "# Contrast weights for Gap Reduction / Equity DiD:\n",
    "# (RN - UN) - (RY - UY)  => +1*RN + (-1)*UN + (-1)*RY + (+1)*UY\n",
    "contrast_groups = [RN, UN, RY, UY]\n",
    "contrast_weights = [1, -1, -1, 1]\n",
    "\n",
    "gap_reduction_test = welch_linear_contrast(contrast_groups, contrast_weights, alpha=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d1cc12-518b-465d-9154-b5200c0c64ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8) Bootstrap CI for Gap Reduction (abs and %)\n",
    "# Why: Likert-ish data can be non-normal; bootstrap is a good sensitivity check.\n",
    "# -----------------------------\n",
    "def bootstrap_gap_reduction(pdf_resp, n_boot=5000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    uy = pdf_resp[(pdf_resp.ai_used==\"Y\") & (pdf_resp.school_group==\"Under\")][\"resp_mean_q1_12\"].to_numpy()\n",
    "    un = pdf_resp[(pdf_resp.ai_used==\"N\") & (pdf_resp.school_group==\"Under\")][\"resp_mean_q1_12\"].to_numpy()\n",
    "    ry = pdf_resp[(pdf_resp.ai_used==\"Y\") & (pdf_resp.school_group==\"Resourced\")][\"resp_mean_q1_12\"].to_numpy()\n",
    "    rn = pdf_resp[(pdf_resp.ai_used==\"N\") & (pdf_resp.school_group==\"Resourced\")][\"resp_mean_q1_12\"].to_numpy()\n",
    "\n",
    "    out_abs = np.empty(n_boot, dtype=float)\n",
    "    out_pct = np.empty(n_boot, dtype=float)\n",
    "\n",
    "    for i in range(n_boot):\n",
    "        mu_rn = rng.choice(rn, size=rn.size, replace=True).mean()\n",
    "        mu_un = rng.choice(un, size=un.size, replace=True).mean()\n",
    "        mu_ry = rng.choice(ry, size=ry.size, replace=True).mean()\n",
    "        mu_uy = rng.choice(uy, size=uy.size, replace=True).mean()\n",
    "\n",
    "        gb = mu_rn - mu_un\n",
    "        ga = mu_ry - mu_uy\n",
    "        gr = gb - ga\n",
    "\n",
    "        out_abs[i] = gr\n",
    "        out_pct[i] = gr / gb if gb != 0 else np.nan\n",
    "\n",
    "    # Percentile CI\n",
    "    ci_abs = np.nanpercentile(out_abs, [2.5, 97.5])\n",
    "    ci_pct = np.nanpercentile(out_pct, [2.5, 97.5])\n",
    "\n",
    "    # Two-sided bootstrap p-value: proportion of bootstrap distribution on the \"wrong\" side\n",
    "    p_abs = 2 * min(np.mean(out_abs <= 0), np.mean(out_abs >= 0))\n",
    "\n",
    "    return {\n",
    "        \"boot_ci_low_abs\": float(ci_abs[0]),\n",
    "        \"boot_ci_high_abs\": float(ci_abs[1]),\n",
    "        \"boot_p_two_sided_abs\": float(p_abs),\n",
    "        \"boot_ci_low_pct\": float(ci_pct[0]),\n",
    "        \"boot_ci_high_pct\": float(ci_pct[1]),\n",
    "    }\n",
    "\n",
    "boot = bootstrap_gap_reduction(pdf, n_boot=5000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08022f03-5722-4a61-aa0b-530e9b52e909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9) Present results\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "results.append({\n",
    "    \"metric\": \"Experience Lift (AI - Baseline)\",\n",
    "    \"estimate\": exp_lift[\"estimate\"],\n",
    "    \"t\": exp_lift[\"t\"],\n",
    "    \"df\": exp_lift[\"df\"],\n",
    "    \"p\": exp_lift[\"p\"],\n",
    "    \"ci_low\": exp_lift[\"ci_low\"],\n",
    "    \"ci_high\": exp_lift[\"ci_high\"],\n",
    "    \"n_A\": sY[\"n\"],\n",
    "    \"n_B\": sN[\"n\"],\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"metric\": \"Equity Gap (Baseline) = RN - UN\",\n",
    "    \"estimate\": gap_baseline,\n",
    "    \"t\": np.nan,\n",
    "    \"df\": np.nan,\n",
    "    \"p\": np.nan,\n",
    "    \"ci_low\": np.nan,\n",
    "    \"ci_high\": np.nan,\n",
    "    \"n_A\": RN[\"n\"],\n",
    "    \"n_B\": UN[\"n\"],\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"metric\": \"Equity Gap (AI) = RY - UY\",\n",
    "    \"estimate\": gap_ai,\n",
    "    \"t\": np.nan,\n",
    "    \"df\": np.nan,\n",
    "    \"p\": np.nan,\n",
    "    \"ci_low\": np.nan,\n",
    "    \"ci_high\": np.nan,\n",
    "    \"n_A\": RY[\"n\"],\n",
    "    \"n_B\": UY[\"n\"],\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"metric\": \"Gap Reduction (abs) = (RN-UN) - (RY-UY)\",\n",
    "    \"estimate\": gap_reduction_test[\"estimate\"],\n",
    "    \"t\": gap_reduction_test[\"t\"],\n",
    "    \"df\": gap_reduction_test[\"df\"],\n",
    "    \"p\": gap_reduction_test[\"p\"],\n",
    "    \"ci_low\": gap_reduction_test[\"ci_low\"],\n",
    "    \"ci_high\": gap_reduction_test[\"ci_high\"],\n",
    "    \"n_A\": RN[\"n\"] + UN[\"n\"] + RY[\"n\"] + UY[\"n\"],\n",
    "    \"n_B\": np.nan,\n",
    "})\n",
    "\n",
    "results.append({\n",
    "    \"metric\": \"Gap Reduction (%) = GapReductionAbs / GapBaseline\",\n",
    "    \"estimate\": gap_reduction_pct,\n",
    "    \"t\": np.nan,\n",
    "    \"df\": np.nan,\n",
    "    \"p\": np.nan,\n",
    "    \"ci_low\": np.nan,\n",
    "    \"ci_high\": np.nan,\n",
    "    \"n_A\": np.nan,\n",
    "    \"n_B\": np.nan,\n",
    "})\n",
    "\n",
    "\n",
    "out = pd.DataFrame(results)\n",
    "\n",
    "# Nicely formatted view\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:0.4f}\" if pd.notnull(x) else \"NaN\")\n",
    "display(out)\n",
    "\n",
    "print(\"\\nBootstrap robustness check (Gap Reduction):\")\n",
    "display(pd.DataFrame([boot]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Statistical test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
